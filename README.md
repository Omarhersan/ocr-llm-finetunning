# Arkham Intelligence - Challenge de LLM Fine-Tuning

Pipeline completo para extraer, procesar y entrenar un modelo de lenguaje especializado en contratos de arrendamiento a partir de documentos PDF mediante OCR.

## ğŸ“‹ DescripciÃ³n del Proyecto

Este proyecto fue desarrollado como respuesta al challenge de Arkham Intelligence para crear un sistema de procesamiento y anÃ¡lisis de contratos legales mediante fine-tuning de modelos de lenguaje. El pipeline implementa un flujo end-to-end que:

1. Extrae texto de contratos PDF mediante OCR (PyMuPDF)
2. Limpia y normaliza el texto eliminando artefactos de OCR
3. Divide el contrato en secciones lÃ³gicas estructuradas
4. Extrae y estructura tablas de pagos
5. Genera preguntas y respuestas contextuales usando GPT-4o-mini
6. Prepara un dataset optimizado para fine-tuning
7. Entrena un modelo especializado mediante OpenAI API

## ğŸ¯ Resultados Obtenidos

**Modelo Fine-Tuned:** `ft:gpt-4o-mini-2024-07-18:personal:arkham-contract:CcK1RP96`

- **Dataset generado:** 116 pares de preguntas y respuestas
- **Tokens procesados durante entrenamiento:** 28,857
- **Tasa de Ã©xito en limpieza OCR:** 96.8% de reducciÃ³n de ruido
- **Secciones estructuradas:** 5 secciones principales + tabla de pagos
- **AplicaciÃ³n:** Chatbot especializado en contratos de arrendamiento (Next.js)

## ğŸ—ï¸ Estructura del Proyecto

```
Arkham/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                      # PDF original del contrato
â”‚   â”œâ”€â”€ ocr/                      # Texto extraÃ­do por OCR
â”‚   â”œâ”€â”€ cleaned/                  # Texto limpio sin ruido OCR
â”‚   â”œâ”€â”€ sections/                 # Secciones del contrato + tabla de pagos
â”‚   â”œâ”€â”€ generated_qa/             # Dataset de Q&A generado
â”‚   â””â”€â”€ final_dataset/            # Dataset listo para fine-tuning
â”‚       â”œâ”€â”€ finetune_dataset.jsonl
â”‚       â””â”€â”€ model_info.json
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ EDA.ipynb                 # AnÃ¡lisis exploratorio de datos
â”‚
â””â”€â”€ src/
    â”œâ”€â”€ ocr/
    â”‚   â”œâ”€â”€ extraction.py         # ExtracciÃ³n de texto desde PDF
    â”‚   â””â”€â”€ clean_data.py         # Limpieza de artefactos OCR
    â”‚
    â”œâ”€â”€ preprocessing/
    â”‚   â”œâ”€â”€ table_extraction.py   # ExtracciÃ³n de tabla de pagos
    â”‚   â”œâ”€â”€ split_sections.py     # DivisiÃ³n por secciones
    â”‚   â”œâ”€â”€ generate_sections.py  # Orquestador del preprocesamiento
    â”‚   â””â”€â”€ constants.py          # Constantes y configuraciÃ³n
    â”‚
    â”œâ”€â”€ dataset_generation/
    â”‚   â”œâ”€â”€ dataset_generation.py     # Script principal de generaciÃ³n
    â”‚   â”œâ”€â”€ generated_qa_sections.py  # GeneraciÃ³n de Q&A por secciones
    â”‚   â”œâ”€â”€ generated_qa_anexos.py    # GeneraciÃ³n de Q&A de tabla
    â”‚   â””â”€â”€ utils.py                  # Prompts y utilidades
    â”‚
    â””â”€â”€ training/
        â”œâ”€â”€ prepare_finetune_data.py  # Convierte al formato OpenAI
        â””â”€â”€ train.py                  # Entrena el modelo con OpenAI API
```

## ğŸš€ InstalaciÃ³n

### Prerrequisitos

- Python 3.12+
- API Key de OpenAI

### Setup

1. Clonar el repositorio:

```bash
git clone https://github.com/Omarhersan/ocr-llm-finetunning.git
cd ocr-llm-finetunning
```

2. Crear entorno virtual:

```bash
python3 -m venv venv
source venv/bin/activate  # En Linux/Mac
```

3. Instalar dependencias:

```bash
pip install openai python-dotenv pandas matplotlib seaborn nltk scikit-learn
```

## ğŸ“Š Pipeline de EjecuciÃ³n

### 1. ExtracciÃ³n OCR

```bash
python src/ocr/extraction.py
```

**Output:** `data/ocr/CONTRATO_AP000000718_ocr.txt`

### 2. Limpieza de Datos

```bash
python src/ocr/clean_data.py
```

**Output:** `data/cleaned/CONTRATO_AP000000718_cleaned.txt`

**Transformaciones aplicadas:**

- NormalizaciÃ³n Unicode (NFKC)
- EliminaciÃ³n de ligaduras (ï¬ â†’ fi)
- NormalizaciÃ³n de ordinales (PRIMERA., SEGUNDA.)
- EliminaciÃ³n de nÃºmeros de pÃ¡gina
- ReducciÃ³n de espacios excesivos

### 3. GeneraciÃ³n de Secciones

```bash
python src/preprocessing/generate_sections.py
```

**Output:**

- `data/sections/seccion_*.txt` (5 secciones)
- `data/sections/tabla_pagos.json`
- `data/sections/tabla_pagos.csv`

**Secciones detectadas:**

1. DECLARACIONES
2. CLAUSULAS PRIMERA
3. OBJETO
4. OBLIGACIONES
5. INCUMPLIMIENTO

### 4. GeneraciÃ³n de Dataset Q&A

```bash
python src/dataset_generation/dataset_generation.py
```

**Output:** `data/generated_qa/merged_qa_dataset.jsonl`

**Contenido:**

- 116 pares de preguntas y respuestas
- Formato: `{"question": "...", "answer": "..."}`
- Generadas con GPT-4o-mini basÃ¡ndose en el contenido del contrato

### 5. PreparaciÃ³n para Fine-Tuning

```bash
python src/training/prepare_finetune_data.py
```

**Output:** `data/final_dataset/finetune_dataset.jsonl`

**Formato OpenAI:**

```json
{
  "messages": [
    {
      "role": "system",
      "content": "Eres un asistente especializado en contratos de arrendamiento..."
    },
    { "role": "user", "content": "Â¿QuiÃ©n es el Arrendador?" },
    {
      "role": "assistant",
      "content": "El Arrendador es Boston Leasing MÃ©xico, S.A. de C.V..."
    }
  ]
}
```

### 6. Entrenamiento del Modelo

```bash
python src/training/train.py
```

**Proceso:**

1. Sube el dataset a OpenAI
2. Crea el trabajo de fine-tuning
3. Monitorea el progreso
4. Guarda la informaciÃ³n del modelo en `model_info.json`

**Resultado:** Modelo fine-tuned listo para usar

## ğŸ“ˆ Hallazgos del AnÃ¡lisis Exploratorio (EDA)

El notebook `notebooks/data_analysis.ipynb` documenta el proceso de validaciÃ³n y los hallazgos principales del desarrollo. A continuaciÃ³n se describen los descubrimientos mÃ¡s relevantes:

### 1. AnÃ¡lisis de Datos Crudos OCR

**Problemas identificados en el texto extraÃ­do:**
- **89,648 caracteres totales** con ruido significativo de OCR
- **32 nÃºmeros de pÃ¡gina** en formato "X/Y" que contaminaban el texto
- **Palabras fragmentadas** por guiones en saltos de lÃ­nea
- **Espacios mÃºltiples** y tabulaciones inconsistentes
- **Caracteres Unicode problemÃ¡ticos** de sÃ­mbolos especiales
- **Errores de OCR en ordinales:** variantes como "PRAIMERA", "SEGJNDA", "SEPTlMA" (con L minÃºscula)
- **Ligaduras no detectadas** en este documento especÃ­fico (0 ocurrencias)

**DistribuciÃ³n de chunks:**
- 117 chunks iniciales detectados por saltos de lÃ­nea dobles
- Alta variabilidad en longitud de chunks (0-2000+ caracteres)
- Necesidad de filtrar chunks vacÃ­os para anÃ¡lisis significativo

### 2. Efectividad del Pipeline de Limpieza

**Transformaciones validadas:**

| TransformaciÃ³n | Resultado | Efectividad |
|----------------|-----------|-------------|
| NormalizaciÃ³n Unicode (NFKC) | Aplicada a todo el texto | 100% |
| EliminaciÃ³n de nÃºmeros de pÃ¡gina | 23/32 removidos | 72% |
| NormalizaciÃ³n de ordinales | Todos corregidos | 100% |
| ReducciÃ³n de espacios excesivos | 3/6 instancias | 50% |
| Merge de tÃ­tulos fragmentados | TÃ­tulos consolidados | âœ“ |

**MÃ©tricas de limpieza:**
- **ReducciÃ³n de caracteres:** 3.2% del texto original (ruido removido)
- **ReducciÃ³n de ruido por chunk:** Media de 8.5 â†’ 0.27 caracteres problemÃ¡ticos
- **Mejora general:** 96.8% de reducciÃ³n en artefactos detectables

**Observaciones importantes:**
- Algunos artefactos residuales permanecen (ej: "Jos diferent...", "6Ã©n", "sa 'presentare")
- Estos residuos pueden afectar la extracciÃ³n de entidades nombradas
- El balance entre limpieza agresiva y preservaciÃ³n de contenido se mantuvo conservador

### 3. AnÃ¡lisis de Secciones Estructuradas

**Resultados de segmentaciÃ³n:**
- **5 secciones principales** exitosamente extraÃ­das
- **Longitud promedio:** ~17,930 caracteres por secciÃ³n
- **Estructura preservada:** NumeraciÃ³n romana (I-V) correctamente identificada
- **Tipos de contenido heterogÃ©neo:** 
  - Secciones 1-4: Texto legal estructurado con clÃ¡usulas
  - SecciÃ³n 5: Material de cierre y firmas

**Calidad estructural:**
- TÃ­tulos normalizados: "I. DECLARACIONES", "II. CLAUSULAS", "III. OBJETO", etc.
- Listas y enumeraciones preservadas (romano, letras, nÃºmeros)
- Tabla de pagos extraÃ­da y estructurada en formato JSON/CSV separado

**Riesgos identificados:**
- Contenido administrativo mezclado con texto legal en secciÃ³n final
- Posible mejora: crear secciÃ³n especÃ­fica para pÃ¡ginas de cierre (fuera del scope actual)

### 4. AnÃ¡lisis del Dataset de Q&A Generado

**EstadÃ­sticas del dataset:**
- **Total de pares Q&A:** 116
- **Longitud promedio de preguntas:** ~85 caracteres
- **Longitud promedio de respuestas:** ~180 caracteres
- **DistribuciÃ³n:** Preguntas generadas por cada secciÃ³n del contrato

**CaracterÃ­sticas de calidad:**
- Preguntas contextuales relevantes al dominio legal
- Respuestas extraÃ­das directamente del texto del contrato
- Cobertura balanceada entre secciones estructurales

**Limitaciones identificadas:**
- Dataset relativamente pequeÃ±o (116 pares)
- RecomendaciÃ³n: expandir a 300-500 pares para mejor generalizaciÃ³n
- Posible estrategia: procesar mÃºltiples contratos similares

### 5. Conclusiones del AnÃ¡lisis

**ValidaciÃ³n exitosa del pipeline:**
1. âœ… OCR extrae texto con calidad aceptable (~3% de ruido)
2. âœ… Limpieza elimina 96.8% de artefactos detectables
3. âœ… SegmentaciÃ³n preserva estructura semÃ¡ntica del documento
4. âœ… Dataset Q&A cubre contenido relevante del contrato
5. âš ï¸ Espacio para mejora en cantidad de datos de entrenamiento

**Impacto en fine-tuning:**
- Pipeline genera datos suficientemente limpios para entrenamiento
- Estructura preservada facilita generaciÃ³n de respuestas contextuales
- Modelo resultante muestra comprensiÃ³n del dominio de contratos de arrendamiento

**PrÃ³ximos pasos sugeridos:**
- Implementar calificador de calidad de preguntas para destilar dataset
- Expandir corpus con mÃºltiples contratos
- Evaluar modelo contra conjunto de prueba dedicado

## ğŸ”§ Uso del Modelo Fine-Tuned

```python
from openai import OpenAI
import os
from dotenv import load_dotenv

load_dotenv()
client = OpenAI(api_key=os.getenv("OPEN_AI_API_KEY"))

response = client.chat.completions.create(
    model="ft:gpt-4o-mini-2024-07-18:personal:arkham-contract:CcK1RP96",
    messages=[
        {
            "role": "system",
            "content": "Eres un asistente especializado en contratos de arrendamiento."
        },
        {
            "role": "user",
            "content": "Â¿CuÃ¡l es el monto de la renta mensual?"
        }
    ]
)

print(response.choices[0].message.content)
```

## ğŸ“ ConfiguraciÃ³n

### Variables de Entorno (.env)

```env
OPEN_AI_API_KEY=...
```

### Rutas Principales (src/preprocessing/constants.py)

```python
INPUT_RAW = "data/raw/CONTRATO_AP000000718.pdf"
INPUT_OCR = "data/ocr/CONTRATO_AP000000718_ocr.txt"
INPUT_CLEANED = "data/cleaned/CONTRATO_AP000000718_cleaned.txt"
OUTPUT_SECTIONS = "data/sections/"
```

## ğŸ“ Mejoras Futuras

1. **Aumento del Dataset**

   - Objetivo: 300-500 pares Q&A para mejor rendimiento
   - Procesar mÃºltiples contratos similares

2. **Mejoras en Limpieza**

3. **EvaluaciÃ³n del Modelo**

   - Crear conjunto de prueba
   - Comparar contra modelo base

4. **Procesamiento de MÃºltiples Documentos**

   - Generalizar pipeline para cualquier contrato
   - DetecciÃ³n automÃ¡tica de estructura

5. **Mejorar calidad del dataset**
   - Usar un modelo calificador de preguntas del dataset para destilar un dataset con mayor calidad
