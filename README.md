# Arkham OCR & LLM Fine-Tuning Pipeline

Pipeline completo para extraer, procesar y entrenar un modelo de lenguaje especializado en contratos de arrendamiento a partir de documentos PDF mediante OCR.

## ğŸ“‹ DescripciÃ³n

Este proyecto implementa un pipeline end-to-end que:

1. Extrae texto de contratos PDF mediante OCR
2. Limpia y normaliza el texto (elimina artefactos, normaliza formatos)
3. Divide el contrato en secciones lÃ³gicas
4. Extrae tablas de pagos y las estructura
5. Genera preguntas y respuestas usando GPT-4o-mini
6. Prepara un dataset para fine-tuning
7. Entrena un modelo especializado con OpenAI

## ğŸ¯ Resultado

**Modelo Fine-Tuned:** `ft:gpt-4o-mini-2024-07-18:personal:arkham-contract:CcK1RP96`

- **Dataset:** 116 pares de preguntas y respuestas
- **Tokens entrenados:** 28,857
- **Uso:** Responder preguntas sobre contratos de arrendamiento

## ğŸ—ï¸ Estructura del Proyecto

```
Arkham/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                      # PDF original del contrato
â”‚   â”œâ”€â”€ ocr/                      # Texto extraÃ­do por OCR
â”‚   â”œâ”€â”€ cleaned/                  # Texto limpio sin ruido OCR
â”‚   â”œâ”€â”€ sections/                 # Secciones del contrato + tabla de pagos
â”‚   â”œâ”€â”€ generated_qa/             # Dataset de Q&A generado
â”‚   â””â”€â”€ final_dataset/            # Dataset listo para fine-tuning
â”‚       â”œâ”€â”€ finetune_dataset.jsonl
â”‚       â””â”€â”€ model_info.json
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ EDA.ipynb                 # AnÃ¡lisis exploratorio de datos
â”‚
â””â”€â”€ src/
    â”œâ”€â”€ ocr/
    â”‚   â”œâ”€â”€ extraction.py         # ExtracciÃ³n de texto desde PDF
    â”‚   â””â”€â”€ clean_data.py         # Limpieza de artefactos OCR
    â”‚
    â”œâ”€â”€ preprocessing/
    â”‚   â”œâ”€â”€ table_extraction.py   # ExtracciÃ³n de tabla de pagos
    â”‚   â”œâ”€â”€ split_sections.py     # DivisiÃ³n por secciones
    â”‚   â”œâ”€â”€ generate_sections.py  # Orquestador del preprocesamiento
    â”‚   â””â”€â”€ constants.py          # Constantes y configuraciÃ³n
    â”‚
    â”œâ”€â”€ dataset_generation/
    â”‚   â”œâ”€â”€ dataset_generation.py     # Script principal de generaciÃ³n
    â”‚   â”œâ”€â”€ generated_qa_sections.py  # GeneraciÃ³n de Q&A por secciones
    â”‚   â”œâ”€â”€ generated_qa_anexos.py    # GeneraciÃ³n de Q&A de tabla
    â”‚   â””â”€â”€ utils.py                  # Prompts y utilidades
    â”‚
    â””â”€â”€ training/
        â”œâ”€â”€ prepare_finetune_data.py  # Convierte al formato OpenAI
        â””â”€â”€ train.py                  # Entrena el modelo con OpenAI API
```

## ğŸš€ InstalaciÃ³n

### Prerrequisitos

- Python 3.12+
- API Key de OpenAI

### Setup

1. Clonar el repositorio:

```bash
git clone https://github.com/Omarhersan/ocr-llm-finetunning.git
cd ocr-llm-finetunning
```

2. Crear entorno virtual:

```bash
python3 -m venv venv
source venv/bin/activate  # En Linux/Mac
```

3. Instalar dependencias:

```bash
pip install openai python-dotenv pandas matplotlib seaborn nltk scikit-learn
```

## ğŸ“Š Pipeline de EjecuciÃ³n

### 1. ExtracciÃ³n OCR

```bash
python src/ocr/extraction.py
```

**Output:** `data/ocr/CONTRATO_AP000000718_ocr.txt`

### 2. Limpieza de Datos

```bash
python src/ocr/clean_data.py
```

**Output:** `data/cleaned/CONTRATO_AP000000718_cleaned.txt`

**Transformaciones aplicadas:**

- NormalizaciÃ³n Unicode (NFKC)
- EliminaciÃ³n de ligaduras (ï¬ â†’ fi)
- NormalizaciÃ³n de ordinales (PRIMERA., SEGUNDA.)
- EliminaciÃ³n de nÃºmeros de pÃ¡gina
- ReducciÃ³n de espacios excesivos

### 3. GeneraciÃ³n de Secciones

```bash
python src/preprocessing/generate_sections.py
```

**Output:**

- `data/sections/seccion_*.txt` (5 secciones)
- `data/sections/tabla_pagos.json`
- `data/sections/tabla_pagos.csv`

**Secciones detectadas:**

1. DECLARACIONES
2. CLAUSULAS PRIMERA
3. OBJETO
4. OBLIGACIONES
5. INCUMPLIMIENTO

### 4. GeneraciÃ³n de Dataset Q&A

```bash
python src/dataset_generation/dataset_generation.py
```

**Output:** `data/generated_qa/merged_qa_dataset.jsonl`

**Contenido:**

- 116 pares de preguntas y respuestas
- Formato: `{"question": "...", "answer": "..."}`
- Generadas con GPT-4o-mini basÃ¡ndose en el contenido del contrato

### 5. PreparaciÃ³n para Fine-Tuning

```bash
python src/training/prepare_finetune_data.py
```

**Output:** `data/final_dataset/finetune_dataset.jsonl`

**Formato OpenAI:**

```json
{
  "messages": [
    {
      "role": "system",
      "content": "Eres un asistente especializado en contratos de arrendamiento..."
    },
    { "role": "user", "content": "Â¿QuiÃ©n es el Arrendador?" },
    {
      "role": "assistant",
      "content": "El Arrendador es Boston Leasing MÃ©xico, S.A. de C.V..."
    }
  ]
}
```

### 6. Entrenamiento del Modelo

```bash
python src/training/train.py
```

**Proceso:**

1. Sube el dataset a OpenAI
2. Crea el trabajo de fine-tuning
3. Monitorea el progreso
4. Guarda la informaciÃ³n del modelo en `model_info.json`

**Resultado:** Modelo fine-tuned listo para usar

## ğŸ“ˆ AnÃ¡lisis Exploratorio (EDA)

El notebook `notebooks/data_analysis.ipynb` contiene anÃ¡lisis detallado:

- **SecciÃ³n 1:** AnÃ¡lisis de datos crudos OCR (ruido, artefactos)
- **SecciÃ³n 2:** ComparaciÃ³n entre datos crudos y limpios
- **SecciÃ³n 3:** AnÃ¡lisis de secciones generadas
- **SecciÃ³n 4:** EstadÃ­sticas del dataset Q&A
- **SecciÃ³n 5:** Resumen y conclusiones

**MÃ©tricas clave:**

- ReducciÃ³n de ruido: 19.67%
- NÃºmeros de pÃ¡gina eliminados: 72% (23/32)
- Espacios excesivos reducidos: 50% (3/6)
- NormalizaciÃ³n de ordinales: 100% de OCR corregidos

## ğŸ”§ Uso del Modelo Fine-Tuned

```python
from openai import OpenAI
import os
from dotenv import load_dotenv

load_dotenv()
client = OpenAI(api_key=os.getenv("OPEN_AI_API_KEY"))

response = client.chat.completions.create(
    model="ft:gpt-4o-mini-2024-07-18:personal:arkham-contract:CcK1RP96",
    messages=[
        {
            "role": "system",
            "content": "Eres un asistente especializado en contratos de arrendamiento."
        },
        {
            "role": "user",
            "content": "Â¿CuÃ¡l es el monto de la renta mensual?"
        }
    ]
)

print(response.choices[0].message.content)
```

## ğŸ“ ConfiguraciÃ³n

### Variables de Entorno (.env)

```env
OPEN_AI_API_KEY=...
```

### Rutas Principales (src/preprocessing/constants.py)

```python
INPUT_RAW = "data/raw/CONTRATO_AP000000718.pdf"
INPUT_OCR = "data/ocr/CONTRATO_AP000000718_ocr.txt"
INPUT_CLEANED = "data/cleaned/CONTRATO_AP000000718_cleaned.txt"
OUTPUT_SECTIONS = "data/sections/"
```

## ğŸ“ Mejoras Futuras

1. **Aumento del Dataset**

   - Objetivo: 300-500 pares Q&A para mejor rendimiento
   - Procesar mÃºltiples contratos similares

2. **Mejoras en Limpieza**

3. **EvaluaciÃ³n del Modelo**

   - Crear conjunto de prueba
   - Comparar contra modelo base

4. **Procesamiento de MÃºltiples Documentos**

   - Generalizar pipeline para cualquier contrato
   - DetecciÃ³n automÃ¡tica de estructura

5. **Mejorar calidad del dataset**
   - Usar un modelo calificador de preguntas del dataset para destilar un dataset con mayor calidad
